{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utility\n",
    "mnist_folder = 'data/mnist'\n",
    "tf.enable_eager_execution()\n",
    "mnist_folder = 'data/mnist'\n",
    "layer1_size = 300\n",
    "layer2_size = 100\n",
    "input_size = 28 * 28\n",
    "output_size = 10  # 1 hot encoder\n",
    "\n",
    "\n",
    "class LeNet300(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet300, self).__init__()\n",
    "        self.layer1 = tf.layers.Dense(layer1_size, activation=tf.nn.relu)\n",
    "        self.layer2 = tf.layers.Dense(layer2_size, activation=tf.nn.relu)\n",
    "        self.out = tf.layers.Dense(output_size)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        return self.out(self.layer2(self.layer1(x)))\n",
    "# Define the loss function\n",
    "def loss(net, x, y):\n",
    "    ypred = net(x)\n",
    "    l = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=ypred))\n",
    "    return l\n",
    "\n",
    "def print_zero_stat(net):\n",
    "    [l1,l1_b] = net.layer1.get_weights()\n",
    "    [l2 , l2_b]= net.layer2.get_weights()\n",
    "    [out, out_b] = net.out.get_weights()\n",
    "    print('Layer1 zeros:',np.count_nonzero(l1==0),' on ',l1.shape[0]*l1.shape[1])\n",
    "    print('Layer1 bias zeros :',np.count_nonzero(l1_b == 0),' on ',l1_b.shape[0])\n",
    "    print('Layer2 zeros :',np.count_nonzero(l2 == 0),' on ',l2.shape[0]*l2.shape[1])\n",
    "    print('Layer2 bias zeros :',np.count_nonzero(l2_b == 0),' on ',l2.shape[0])\n",
    "    print('Out zeros :',np.count_nonzero(out == 0),' on ',out.shape[0]*out.shape[1])\n",
    "    print('Out bias zeros :',np.count_nonzero(out_b == 0),' on ',out_b.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before quantization \n",
      "accuracy:  0.9721\n",
      "Layer1 zeros: 230068  on  235200\n",
      "Layer1 bias zeros : 300  on  300\n",
      "Layer2 zeros : 28566  on  30000\n",
      "Layer2 bias zeros : 97  on  300\n",
      "Out zeros : 777  on  1000\n",
      "Out bias zeros : 10  on  10\n"
     ]
    }
   ],
   "source": [
    "# get the pruned NN\n",
    "[(xtrain, ytrain), (xval, yval), (xtest, ytest)] = utility.read_mnist(mnist_folder, flatten=True,num_train=-1)\n",
    "net = LeNet300()\n",
    "loss_grad = tfe.implicit_value_and_gradients(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001)\n",
    "root = tf.train.Checkpoint(optimizer=opt,\n",
    "                           model=net,\n",
    "                           optimizer_step=tf.train.get_or_create_global_step())\n",
    "root.restore(tf.train.latest_checkpoint('checkpoint-lenet300'))\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((xtrain, ytrain)).shuffle(1000).batch(32, drop_remainder=True)\n",
    "ypred = tf.nn.softmax(net(tf.constant(xtest)))\n",
    "ypred = tf.argmax(ypred, axis=1)\n",
    "\n",
    "ytest = tf.argmax(ytest, axis=1)\n",
    "tmp = tf.cast(tf.equal(ypred, ytest), tf.float32)\n",
    "print('before quantization \\naccuracy: ', tf.reduce_mean(tmp).numpy())\n",
    "print_zero_stat(net)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1_cls = np.copy(w1)\n",
    "sx=w1.shape[0]\n",
    "sy=w1.shape[1]\n",
    "for i in range(sx):\n",
    "    for j in range(sy):\n",
    "        w1_cls[i,j] = kmeans.cluster_centers_[lab[i*j]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "230946\n"
     ]
    }
   ],
   "source": [
    "a,_=np.nonzero(w1_cls == cc[0])\n",
    "print(len(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cluster_to_gradient(gradient,cluster,label):\n",
    "    new_gradient = np.copy(gradient)\n",
    "    sx=new_gradient.shape[0]\n",
    "    sy=new_gradient.shape[1]\n",
    "    grad_dic={}\n",
    "    for i in range(sx):\n",
    "        for j in range(sy):\n",
    "            if (label[i*j] not in grad_dic):\n",
    "                grad_dic[label[i*j]]=gradient[i,j]\n",
    "            else:\n",
    "                grad_dic[label[i*j]]+=gradient[i,j]\n",
    "    for i in range(sx):\n",
    "        for j in range(sy):\n",
    "            new_gradient[i,j] = grad_dic[label[i*j]]\n",
    "                \n",
    "    return new_gradient\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(235200, 1)\n"
     ]
    }
   ],
   "source": [
    "aaa=0\n",
    "#net.set_weights([w1_cls,w2])\n",
    "for Xtrain_t, ytrain_t in tfe.Iterator(train_data):\n",
    "    \n",
    "    w1 = net.layer1.get_weights()[0]\n",
    "    \n",
    "    w_mod=w1.reshape(-1,1)\n",
    "    print(w_mod.shape)\n",
    "    break\n",
    "    kmeans = KMeans(n_clusters=50, random_state=0)\n",
    "    kmeans.fit(w_mod)\n",
    "    cc=kmeans.cluster_centers_\n",
    "    lab = kmeans.labels_\n",
    "    w1_cls = np.copy(w1)\n",
    "    #tf.Variable(w1_cls)\n",
    "    \n",
    "    sx=w1.shape[0]\n",
    "    sy=w1.shape[1]\n",
    "    for i in range(sx):\n",
    "        for j in range(sy):\n",
    "            w1_cls[i,j] = kmeans.cluster_centers_[lab[i*j]]\n",
    "\n",
    "\n",
    "    \n",
    "    print(aaa)\n",
    "    current_loss, grads = loss_grad(net, Xtrain_t, ytrain_t)\n",
    "    #print(grads[0])\n",
    "    print('a')\n",
    "    new_grad=apply_cluster_to_gradient(grads[0][0],cc,lab)\n",
    "    print('b')\n",
    "    net.set_weights([w1_cls,w2])\n",
    "    grads[0]=(new_grad,tf.Variable(w1_cls))\n",
    "    \n",
    "    opt.apply_gradients(grads)\n",
    "    break\n",
    "#opt.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after quantization \n",
      "accuracy:  0.0974\n",
      "Layer1 zeros: 0  on  235200\n",
      "Layer1 bias zeros : 36  on  300\n",
      "Layer2 zeros : 4146  on  30000\n",
      "Layer2 bias zeros : 9  on  300\n",
      "Out zeros : 56  on  1000\n",
      "Out bias zeros : 0  on  10\n"
     ]
    }
   ],
   "source": [
    "ypred = tf.nn.softmax(net(tf.constant(xtest)))\n",
    "ypred = tf.argmax(ypred, axis=1)\n",
    "tmp = tf.cast(tf.equal(ypred, ytest), tf.float32)\n",
    "print('after quantization \\naccuracy: ', tf.reduce_mean(tmp).numpy())\n",
    "print_zero_stat(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00314884 -0.00314884 -0.00314884 ... -0.00314884 -0.00314884\n",
      "  -0.00314884]\n",
      " [-0.00314884 -0.00314884 -0.00314884 ... -0.00314884 -0.00314884\n",
      "  -0.00314884]\n",
      " [-0.00314884 -0.00314884 -0.00314884 ... -0.00314884 -0.00314884\n",
      "  -0.00314884]\n",
      " ...\n",
      " [-0.00314884 -0.00314884 -0.00314884 ... -0.00314884 -0.00314884\n",
      "  -0.00314884]\n",
      " [-0.00314884 -0.00314884 -0.00314884 ... -0.00314884 -0.00314884\n",
      "  -0.00314884]\n",
      " [-0.00314884 -0.00314884 -0.00314884 ... -0.00314884 -0.00314884\n",
      "  -0.00314884]]\n"
     ]
    }
   ],
   "source": [
    "print(net.get_weights()[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
