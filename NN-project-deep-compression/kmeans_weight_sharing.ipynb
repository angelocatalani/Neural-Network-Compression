{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.contrib.eager as tfe\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import utility\n",
    "mnist_folder = 'data/mnist'\n",
    "tf.enable_eager_execution()\n",
    "mnist_folder = 'data/mnist'\n",
    "layer1_size = 300\n",
    "layer2_size = 100\n",
    "input_size = 28 * 28\n",
    "output_size = 10  # 1 hot encoder\n",
    "\n",
    "\n",
    "class LeNet300(tf.keras.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(LeNet300, self).__init__()\n",
    "        self.layer1 = tf.layers.Dense(layer1_size, activation=tf.nn.relu)\n",
    "        self.layer2 = tf.layers.Dense(layer2_size, activation=tf.nn.relu)\n",
    "        self.out = tf.layers.Dense(output_size)\n",
    "\n",
    "    def call(self, x, **kwargs):\n",
    "        return self.out(self.layer2(self.layer1(x)))\n",
    "# Define the loss function\n",
    "def loss(net, x, y):\n",
    "    ypred = net(x)\n",
    "    l = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=ypred))\n",
    "    return l\n",
    "\n",
    "def print_zero_stat(net):\n",
    "    [l1,l1_b] = net.layer1.get_weights()\n",
    "    [l2 , l2_b]= net.layer2.get_weights()\n",
    "    [out, out_b] = net.out.get_weights()\n",
    "    print('Layer1 zeros:',np.count_nonzero(l1==0),' on ',l1.shape[0]*l1.shape[1])\n",
    "    print('Layer1 bias zeros :',np.count_nonzero(l1_b == 0),' on ',l1_b.shape[0])\n",
    "    print('Layer2 zeros :',np.count_nonzero(l2 == 0),' on ',l2.shape[0]*l2.shape[1])\n",
    "    print('Layer2 bias zeros :',np.count_nonzero(l2_b == 0),' on ',l2.shape[0])\n",
    "    print('Out zeros :',np.count_nonzero(out == 0),' on ',out.shape[0]*out.shape[1])\n",
    "    print('Out bias zeros :',np.count_nonzero(out_b == 0),' on ',out_b.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before quantization \n",
      "accuracy:  0.9721\n",
      "Layer1 zeros: 230068  on  235200\n",
      "Layer1 bias zeros : 300  on  300\n",
      "Layer2 zeros : 28566  on  30000\n",
      "Layer2 bias zeros : 97  on  300\n",
      "Out zeros : 777  on  1000\n",
      "Out bias zeros : 10  on  10\n"
     ]
    }
   ],
   "source": [
    "# get the pruned NN\n",
    "[(xtrain, ytrain), (xval, yval), (xtest, ytest)] = utility.read_mnist(mnist_folder, flatten=True,num_train=-1)\n",
    "net = LeNet300()\n",
    "loss_grad = tfe.implicit_value_and_gradients(loss)\n",
    "opt = tf.train.AdamOptimizer(0.001)\n",
    "root = tf.train.Checkpoint(optimizer=opt,\n",
    "                           model=net,\n",
    "                           optimizer_step=tf.train.get_or_create_global_step())\n",
    "root.restore(tf.train.latest_checkpoint('checkpoint-lenet300'))\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((xtrain, ytrain)).shuffle(1000).batch(32, drop_remainder=True)\n",
    "ypred = tf.nn.softmax(net(tf.constant(xtest)))\n",
    "ypred = tf.argmax(ypred, axis=1)\n",
    "\n",
    "ytest = tf.argmax(ytest, axis=1)\n",
    "tmp = tf.cast(tf.equal(ypred, ytest), tf.float32)\n",
    "print('before quantization \\naccuracy: ', tf.reduce_mean(tmp).numpy())\n",
    "print_zero_stat(net)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cluster_to_gradient(gradient,cluster,label):\n",
    "    new_gradient = np.copy(gradient)\n",
    "    sx=new_gradient.shape[0]\n",
    "    sy=new_gradient.shape[1]\n",
    "    grad_dic={}\n",
    "    for i in range(sx):\n",
    "        for j in range(sy):\n",
    "            if (label[i*j] not in grad_dic):\n",
    "                grad_dic[label[i*j]]=gradient[i,j]\n",
    "            else:\n",
    "                grad_dic[label[i*j]]+=gradient[i,j]\n",
    "    for i in range(sx):\n",
    "        for j in range(sy):\n",
    "            new_gradient[i,j] = grad_dic[label[i*j]]\n",
    "                \n",
    "    return new_gradient\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for Xtrain_t, ytrain_t in tfe.Iterator(train_data):\n",
    "    \n",
    "    #w1 = net.layer1.get_weights()[0]\n",
    "    #w2 = net.layer1.get_weights()[1]\n",
    "    \n",
    "    #w_mod=w1.reshape(-1,1)\n",
    "    #print(w_mod.shape)\n",
    "    #kmeans = KMeans(n_clusters=50, random_state=0)\n",
    "    #kmeans.fit(w_mod)\n",
    "    #cc=kmeans.cluster_centers_\n",
    "    #lab = kmeans.labels_\n",
    "    #w1_cls = np.copy(w1)\n",
    "    #tf.Variable(w1_cls)\n",
    "    \n",
    "    sx=w1.shape[0]\n",
    "    sy=w1.shape[1]\n",
    "    for i in range(sx):\n",
    "        for j in range(sy):\n",
    "            w1_cls[i,j] = kmeans.cluster_centers_[lab[i*j]]\n",
    "    #current_loss, grads = loss_grad(net, Xtrain_t, ytrain_t)\n",
    "    #print(grads[0])\n",
    "    print('a')\n",
    "    #new_grad=apply_cluster_to_gradient(grads[0][0],cc,lab)\n",
    "    print('b')\n",
    "    net.set_weights([w1,w2])\n",
    "    #grads[0]=(new_grad,tf.contrib.eager.Variable(w1_cls))\n",
    "    \n",
    "    #opt.apply_gradients(grads)\n",
    "    break\n",
    "#opt.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 300)\n",
      "(300, 100)\n",
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "w1 = net.layer1.get_weights()[0]\n",
    "w2 = net.layer1.get_weights()[1]\n",
    "\n",
    "w3 = net.layer2.get_weights()[0]\n",
    "w4 = net.layer2.get_weights()[1]\n",
    "\n",
    "w5 = net.out.get_weights()[0]\n",
    "w6 = net.out.get_weights()[1]\n",
    "bits=5\n",
    "\n",
    "\n",
    "min_ = w1.min()\n",
    "max_ = w1.max()\n",
    "space = np.linspace(min_, max_, num=2**bits)\n",
    "kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, precompute_distances=True, algorithm=\"full\")\n",
    "kmeans.fit(w1.reshape(-1,1))\n",
    "new_weight1 = kmeans.cluster_centers_[kmeans.labels_].reshape(w1.shape)\n",
    "print(new_weight1.shape)\n",
    "\n",
    "\n",
    "min_ = w3.min()\n",
    "max_ = w3.max()\n",
    "space = np.linspace(min_, max_, num=2**bits)\n",
    "kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, precompute_distances=True, algorithm=\"full\")\n",
    "kmeans.fit(w3.reshape(-1,1))\n",
    "new_weight3 = kmeans.cluster_centers_[kmeans.labels_].reshape(w3.shape)\n",
    "print(new_weight3.shape)\n",
    "\n",
    "\n",
    "min_ = w5.min()\n",
    "max_ = w5.max()\n",
    "space = np.linspace(min_, max_, num=2**bits)\n",
    "kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, precompute_distances=True, algorithm=\"full\")\n",
    "kmeans.fit(w5.reshape(-1,1))\n",
    "new_weight5 = kmeans.cluster_centers_[kmeans.labels_].reshape(w5.shape)\n",
    "print(new_weight5.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.layer1.set_weights([new_weight1,w2])\n",
    "net.layer2.set_weights([new_weight3,w4])\n",
    "net.out.set_weights([new_weight5,w6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "'''\n",
    "\n",
    "\n",
    "w1 = net.layer1.get_weights()[0]\n",
    "w2 = net.layer1.get_weights()[1]\n",
    "\n",
    "bits=5\n",
    "min_ = min(w1)\n",
    "max_ = max(w1)\n",
    "space = np.linspace(min_, max_, num=2**bits)\n",
    "kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, precompute_distances=True, algorithm=\"full\")\n",
    "kmeans.fit(w1.reshape(-1,1))\n",
    "\n",
    "for Xtrain_t, ytrain_t in tfe.Iterator(train_data):\n",
    "    \n",
    "    #w1 = net.layer1.get_weights()[0]\n",
    "    #w2 = net.layer1.get_weights()[1]\n",
    "    \n",
    "    #w_mod=w1.reshape(-1,1)\n",
    "    #print(w_mod.shape)\n",
    "    #kmeans = KMeans(n_clusters=50, random_state=0)\n",
    "    #kmeans.fit(w_mod)\n",
    "    #cc=kmeans.cluster_centers_\n",
    "    #lab = kmeans.labels_\n",
    "    #w1_cls = np.copy(w1)\n",
    "    #tf.Variable(w1_cls)\n",
    "    \n",
    "    sx=w1.shape[0]\n",
    "    sy=w1.shape[1]\n",
    "    for i in range(sx):\n",
    "        for j in range(sy):\n",
    "            w1_cls[i,j] = kmeans.cluster_centers_[lab[i*j]]\n",
    "    #current_loss, grads = loss_grad(net, Xtrain_t, ytrain_t)\n",
    "    #print(grads[0])\n",
    "    print('a')\n",
    "    #new_grad=apply_cluster_to_gradient(grads[0][0],cc,lab)\n",
    "    print('b')\n",
    "    net.set_weights([w1,w2])\n",
    "    #grads[0]=(new_grad,tf.contrib.eager.Variable(w1_cls))\n",
    "    \n",
    "    #opt.apply_gradients(grads)\n",
    "    break\n",
    "#opt.apply_gradients(grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after quantization \n",
      "accuracy:  0.9679\n",
      "Layer1 zeros: 0  on  235200\n",
      "Layer1 bias zeros : 0  on  300\n",
      "Layer2 zeros : 0  on  30000\n",
      "Layer2 bias zeros : 6  on  300\n",
      "Out zeros : 0  on  1000\n",
      "Out bias zeros : 0  on  10\n"
     ]
    }
   ],
   "source": [
    "ypred = tf.nn.softmax(net(tf.constant(xtest)))\n",
    "ypred = tf.argmax(ypred, axis=1)\n",
    "tmp = tf.cast(tf.equal(ypred, ytest), tf.float32)\n",
    "print('after quantization \\naccuracy: ', tf.reduce_mean(tmp).numpy())\n",
    "print_zero_stat(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00109041 -0.00109041 -0.00109041 ... -0.00109041 -0.00109041\n",
      "  -0.00109041]\n",
      " [-0.00109041 -0.00109041 -0.00109041 ... -0.00109041 -0.00109041\n",
      "  -0.00109041]\n",
      " [-0.00109041 -0.00109041 -0.00109041 ... -0.00109041 -0.00109041\n",
      "  -0.00109041]\n",
      " ...\n",
      " [-0.00109041 -0.00109041 -0.00109041 ... -0.00109041 -0.00109041\n",
      "  -0.00109041]\n",
      " [-0.00109041 -0.00109041 -0.00109041 ... -0.00109041 -0.00109041\n",
      "  -0.00109041]\n",
      " [-0.00109041 -0.00109041 -0.00109041 ... -0.00109041 -0.00109041\n",
      "  -0.00109041]]\n"
     ]
    }
   ],
   "source": [
    "print(net.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:971: ConvergenceWarning: Number of distinct clusters (32) found smaller than n_clusters (50). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n"
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=50, random_state=0)\n",
    "kmeans.fit(w1.reshape((-1,1)))\n",
    "new_weight = kmeans.cluster_centers_[kmeans.labels_].reshape(784,300)\n",
    "net.layer1.set_weights([new_weight,w2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after quantization \n",
      "accuracy:  0.9679\n",
      "Layer1 zeros: 0  on  235200\n",
      "Layer1 bias zeros : 0  on  300\n",
      "Layer2 zeros : 0  on  30000\n",
      "Layer2 bias zeros : 6  on  300\n",
      "Out zeros : 0  on  1000\n",
      "Out bias zeros : 0  on  10\n"
     ]
    }
   ],
   "source": [
    "ypred = tf.nn.softmax(net(tf.constant(xtest)))\n",
    "ypred = tf.argmax(ypred, axis=1)\n",
    "tmp = tf.cast(tf.equal(ypred, ytest), tf.float32)\n",
    "print('after quantization \\naccuracy: ', tf.reduce_mean(tmp).numpy())\n",
    "print_zero_stat(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00107697 -0.00107697 -0.00107697 ... -0.00107697 -0.00107697\n",
      "  -0.00107697]\n",
      " [-0.00107697 -0.00107697 -0.00107697 ... -0.00107697 -0.00107697\n",
      "  -0.00107697]\n",
      " [-0.00107697 -0.00107697 -0.00107697 ... -0.00107697 -0.00107697\n",
      "  -0.00107697]\n",
      " ...\n",
      " [-0.00107697 -0.00107697 -0.00107697 ... -0.00107697 -0.00107697\n",
      "  -0.00107697]\n",
      " [-0.00107697 -0.00107697 -0.00107697 ... -0.00107697 -0.00107697\n",
      "  -0.00107697]\n",
      " [-0.00107697 -0.00107697 -0.00107697 ... -0.00107697 -0.00107697\n",
      "  -0.00107697]]\n"
     ]
    }
   ],
   "source": [
    "print(net.get_weights()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
