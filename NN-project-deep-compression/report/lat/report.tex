\documentclass[journal]{vgtc}                % final (journal style)
%\documentclass[review,journal]{vgtc}         % review (journal style)
%\documentclass[widereview]{vgtc}             % wide-spaced review
%\documentclass[preprint,journal]{vgtc}       % preprint (journal style)
%\documentclass[electronic,journal]{vgtc}     % electronic version, journal

%% Uncomment one of the lines above depending on where your paper is
%% in the conference process. ``review'' and ``widereview'' are for review
%% submission, ``preprint'' is for pre-publication, and the final version
%% doesn't use a specific qualifier. Further, ``electronic'' includes
%% hyperreferences for more convenient online viewing.

%% Please use one of the ``review'' options in combination with the
%% assigned online id (see below) ONLY if your paper uses a double blind
%% review process. Some conferences, like IEEE Vis and InfoVis, have NOT
%% in the past.

%% Please note that the use of figures other than the optional teaser is not permitted on the first page
%% of the journal version.  Figures should begin on the second page and be
%% in CMYK or Grey scale format, otherwise, colour shifting may occur
%% during the printing process.  Papers submitted with figures other than the optional teaser on the
%% first page will be refused.

%% These three lines bring in essential packages: ``mathptmx'' for Type 1
%% typefaces, ``graphicx'' for inclusion of EPS figures. and ``times''
%% for proper handling of the times font family.

\usepackage{mathptmx}
\usepackage{graphicx}
\usepackage{times}
\usepackage{float}



%% We encourage the use of mathptmx for consistent usage of times font
%% throughout the proceedings. However, if you encounter conflicts
%% with other math-related packages, you may want to disable it.

%% This turns references into clickable hyperlinks.
\usepackage[bookmarks,backref=true,linkcolor=black]{hyperref} %,colorlinks
\hypersetup{
  pdfauthor = {},
  pdftitle = {},
  pdfsubject = {},
  pdfkeywords = {},
  colorlinks=true,
  linkcolor= black,
  citecolor= black,
  pageanchor=true,
  urlcolor = black,
  plainpages = false,
  linktocpage
}
\usepackage[hyphenbreaks]{breakurl}

%% If you are submitting a paper to a conference for review with a double
%% blind reviewing process, please replace the value ``0'' below with your
%% OnlineID. Otherwise, you may safely leave it at ``0''.
\onlineid{0}

%% declare the category of your paper, only shown in review mode


%% allow for this line if you want the electronic option to work properly


%% In preprint mode you may define your own headline.
%\preprinttext{To appear in an IEEE VGTC sponsored conference.}

%% Paper title.

\title{DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING AND QUANTIZATION}
%\subtitle{Neural Network project, A.Y. 2018/2019}

%% This is how authors are specified in the journal style

%% indicate IEEE Member or Student Member in form indicated below
\author{Angelo Catalani 1582230}
\abstract{
Neural Network are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources.\\Compressing a neural network with pruning and quantization with no loss of accuracy is the aim of this project }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%% START OF THE PAPER %%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  \teaser{
% \centering
% \includegraphics[width=16cm]{}
%  \caption{The whole project in a picture}
%  }
\begin{document}

%% The ``\maketitle'' command must be the first command after the
%% ``\begin{document}'' command. It prepares and prints the title block.

%% the only exception to this rule is the \firstsection command
\maketitle
\section{Introduction}
I have taken into consideration the following papers:
\begin{enumerate}
\item \cite{p1} : deals with specific pruning issues: regularization terms, threshold choice, and  parameter co-adaption
\item \cite{p2} : is the sequel of the previous paper : pruning and quantization
\end{enumerate}
I have run my experiment with the following two neural networks on the MNIST classification problem: 
\begin{enumerate}
\item LeNet300-100 : two dense layers 
\item LeNet5 : two convolutional layers followed by a dense layer
\end{enumerate}
The loss function is the categorical cross entropy with L2 regularization and the optimizer algorithm is Adam.
\section{Pruning}
The pruning technique I have implemented as described in the papers consists of three steps :
\begin{enumerate}
\item train connectivity : the neural network is trained for a given number of epochs
\item prune connection : remove all the weights below a threshold
\item train the weights : re-train the reduced neural network to learn the final weights and repeat from step 2
\end{enumerate}
It is significant to note that:
\begin{enumerate}
\item the first step is conceptually different from the way a neural network is normally trained because in this step we are interested in finding the important connection rather than the final weights
\item retraining the pruned neural network is necessary for the accuracy since after the remotion of some connection (step 2),in general the accuracy will drop
\item pruning works under the hypothesis that the network is over-parametrized so that it solves not only memory/complexity issues but also can reduce the risk of overfitting
\end{enumerate}
The regularization terms used in the loss function, tends to lower the magnitude of the weight matrices, so that more weights will be close to zero and good candidates for being pruned.\\In particular, I have chosen L2 regularization because gives better results (\cite{p1}).\\Deep neural network can be affected by the vanishing gradient problem.Even if \cite{p1} does not deals directly with that problem it notes that when a neural network struggles to update its weights consistently, it will not be able to recover in terms of accuracy and the training will be ineffective after the pruning of some neurons. \\To deals with this issue, as written in  \cite{p1} I have pruned the convolutional layers and the dense layers on different iterations, so that the error caused by the pruning at each iteration is attenuated.\\In \cite{p1}, the threshold value is obtained as a quality parameter multiplied by the standard deviation of a layer's weights.\\This choice is justified by the fact that as it is the case of my experiments, the weights of a dense/convolutional layers are distributed as a gaussian of zero mean so that the weights in the range of the positive and negative standard deviation are 68\% of the total.\
\section{Quantization And Weight Sharing}
This section is described in \cite{p2}.\\After the network has been pruned, to the network is further compressed by reducing the number of bits to represent a a single weights.\\In particular I have applied k-means (1 dimensional) to the weights of each layer so that the new weights of the layer are the centroids to which the original weights belong to.\\Crucial to this step is the choice of k and the centroid initialization.\\The choice of k is a consequence of the number of bits used in this step : if we want to compress the layers weights to n bits, we can use up to $2^{n}$ centroids.\\The tradeoff between accuracy-compression is due to the number of bits used : the more the bits, the more the accuracy, the more the space required.\\The paper describes 3 different technique :
\begin{enumerate}
\item BLABLABLABLABLA EXPLAIN WITH FIGURES
\item 
\item BLABLABLABLABLA
\end{enumerate}
I have not implemented the fine tuning of the centroids because:
\begin{enumerate}
\item the loss of accuracy is inexistent
\item the latency is eccessive : for each single batch in every epoch, I should have scanned all the gradients (more than 1 minutes for a single batch on my pc) 
\end{enumerate}
\section{Experiment}
1) AGGRESSIVE COMPRESSION WITHOUT STD
2) WITH STD


\begin{thebibliography}{2}

\bibitem{p1}
  Song Han, Jeff Pool, John Tran, William J. Dally
  \textit{Learning both Weights and Connections for Efficient Neural Networks}
  
 \bibitem{p2}
  Song Han, Huizi Mao, John Tran, J. Dally
  \textit{DEEP COMPRESSION: COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING}



\end{thebibliography}
\end{document}
